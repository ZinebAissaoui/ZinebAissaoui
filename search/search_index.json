{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About","text":"Zineb Aissaoui Business Intelligence and Data Engineer | Centralien Engineer  Data transformation &amp; Visualisation  | Solutions Cloud - Azure | Interested at NLP &amp; Machine Learning   Education <ul> <li> Jan 2022 - Jul 2022 International exchange <p>Ecole Centrale Sup\u00e9lecFrance, Paris</p> </li> <li> Sept 2020 - Sept 2024 Data Science and Digitalization <p>Ecole Centrale CasablancaMorocco, Casablanca</p> </li> <li> Sept 2018 - Apr 2020 Mathematics &amp; Physics Engineering Science <p>Preparatory Classes Omar Ibn AbdelazizMorocco, Oujda</p> </li> </ul> Export Resume  About me <p>\ud83d\udd0d Turning data into strategic decisions        Welcome! I\u2019m Zineb Aissaoui, a passionate Data engineer and BI dedicated to uncovering insights and delivering impactful solutions through data. With a strong engineering background in Data science and digitalization at (\u00c9cole Centrale Casablanca and Supel\u00e8c) with expertise in project management, I merge advanced technical expertise with strategic thinking to solve complex business challenges. \ud83d\udca1 What I do: <ul> <li>Design and Build Scalable Data Pipelines: Extract, transform, and load (ETL) data into data warehouses for seamless analysis and storag       <li>Develop Insightful Dashboards: Enable teams to uncover actionable insights and make data-driven strategic decisions.       <li>Construct Predictive Models: Enhance decision-making by forecasting trends and optimizing business strategies.       <li>Automate Big Data Workflows: Ensure efficiency, reliability, and scalability in data processing with automated pipelines.       <li>Leverage advanced technologies: Implement solutions using Generative AI (LLM, RAG), Natural Language Processing (NLP), Computer Vision, and Cloud Computing platforms (Azure, GCP)..</li> \ud83c\udf1f What sets me apart: <ul> <li>I'm Generalistic ingeneer wich makes my sence of adaptability to different sectors very important.       <li>I had 3 experiences in differents sectors: Digital Marketing, construction and It compagny       <li>Fast learner and very curious person       <li>Empowering businesses by automating repetitive and time-intensive tasks.       <li>Very good competences in project management and redaction of cahier de charges and suivie des projets et test et deployment.</li> \ud83c\udfaf My mission?  To collaborate with innovative teams and tackle challenges in big data management, applied AI, and process optimization. \ud83c\udf31 What drives me: Innovation, continuous learning, and creating meaningful impact through data.         Experience <p>BI &amp; Data Engineer Mar 2024- Sep 2024Wapsi ESN - Internship</p> <p> <ul> <li>Integrated scraping features to collect data from external sources and automate workflows.         <li>Scripting with python for data transformation and automation of the process.         <li>Designed and implemented a prospecting app using Power Automate, PowerApps, and Azure Functions.         <li>Used GenAI solutions to add features to the developed solution.         </li> Librairies &amp; Techniques :           \ud83d\udee0\ufe0fPython : Pandas, ReGex.           \ud83d\udee0\ufe0fAzure : Azure Functions, Azure Synapse, Azure Databricks, Azure Datafactory, Azure Blob Storage, Azure Machine Learning, Azure Data studio           \ud83d\udee0\ufe0fPower Microsoft : Power Apps, Power BI, Power Automate          <p> Data Project Manager Jan 2023 - Jul 2023Numberly - Internship</p> <p> <ul> <li>Collaborated with cross-functional teams to gather requirements and define project scope, resulting in successful delivery of data solutions and data pipelines.         <li>Development and implementation of a data visualization dashboard, providing clients with real-time insights and improving decision-making processes.         <li>Expression of requirements of Marketing team to IT Departments.         <li>Optimize an internal search engine by integrating semantics.         <li>Implement summary models to shorten the length of texts in documents. (NLP)         </li> Librairies &amp; Techniques :             \ud83d\udee0\ufe0fSalesforce : Datorama              \ud83d\udee0\ufe0fProject management :  Kanban, Gant diagram .... <p>Consultant Data &amp; Business intelligence Jully 2022 - Dec 2023Vinci Construction Internship</p> <p>Reduction of CO2 emissions due to the use of concrete from purchase invoices :       <ul> <li>Implementation of new Business Intelligence and Data solutions         <li>Data modeling, ETL and warehousing.         <li>Data vizualization : Creation and development of dashboards with PowerBI.         <li>Identifying, extracting and reconciling relevant data from various systems and databases, checking their quality and consistency.         </li> Libraries &amp; techniques :              \ud83d\udee0\ufe0fAzure : Data Factory, Azure Blob Storage             \ud83d\udee0\ufe0fVizualization : PowerBI             \ud83d\udee0\ufe0fSSMS and Microsoft Analysis         Academic Project  <p>RAG for Access to Scientific Articles Feb 2024 - Mar 2024Ecole Centrale Casablanca &amp; AxIA <ul> <li>Objective: Facilitate LLMs' access to scientific articles for advanced interaction.         <li>Benchmarking of LLMs (Gemini, GPT, T5) and vector databases (Chroma, Vectorstore Index).         <li>PDF parsing and use of Langchain and LlamaIndex libraries.         <li>Results: Improved integration of LLMs with scientific databases.         <li>Directed by: Yan LeCun         </li> Code of Project Poster of the project  <p>Asynchronous Architecture for Medical Records Processing and Metadata Retrieval Sep 2023 - Feb 2024Ecole Centrale Casablanca <ul> <li>Objective: Extract Data from Medical prescriptions and reports, using OCR and NLP         <li>Utilization of Artificial Intelligence, specifically Natural Language Processing (NLP), to instantly process new medical content and store essential metadata.         <li>Enhancing medical research through a dedicated search engine.         <li>Development of a search engine to help doctors easily access their patients' medical history.         </li> Technologies: Python, NLP, OCR, TF-IDF, Embeeding... Code of the project  Poster of the project  <p>Realtime Data streamin Nov 2024 - Dec 2024Autonomy <p>An end to end project for Data Streamin </p> <ul> <li>Setting up a data pipeline with Apache Airflow.         <li>Streaming data with Kafka and Kafka Connect.         <li>Using Zookeeper for distributed synchronization.         <li>Data processing with Apache Spark.         <li>Data storage solutions with Cassandra and PostgreSQL.         <li>Containerizing your data engineering environment with Docker.         </li> Technologies:  Apache Airflow, Python, Apache Kafka, Apache Zookeeper, Apache Spark, and Cassandra"},{"location":"projects/Academic%20Projects/Medical%20records%20-%20NLP/","title":"Extraction, Recognition, and Search in Medical Records","text":"<p>This code was developed to automate the management of medical records by performing text extraction, named entity recognition, and enabling precise searches within documents. </p>"},{"location":"projects/Academic%20Projects/Medical%20records%20-%20NLP/#how-the-code-works","title":"How the Code Works","text":""},{"location":"projects/Academic%20Projects/Medical%20records%20-%20NLP/#1-text-extraction","title":"1. Text Extraction","text":"<p>The code processes medical records in PDF format and detects the presence of scanned images. For native PDFs, it uses the pdfplumber library from PyPDF2 to extract textual data. For scanned PDFs, Optical Character Recognition (OCR) is performed using the doctr library. The medical records are then stored as <code>.txt</code> files for further manipulation.</p>"},{"location":"projects/Academic%20Projects/Medical%20records%20-%20NLP/#2-named-entity-recognition-ner","title":"2. Named Entity Recognition (NER)","text":"<ul> <li>For blood test reports, the code uses the Quaero corpus to train a Conditional Random Fields (CRF) Tagger to identify relevant named entities.</li> <li>For other types of documents, regular expressions (Regex) are used to recognize named entities. The extracted metadata is structured using a predefined JSON schema, and the information is stored in a JSON database.</li> </ul>"},{"location":"projects/Academic%20Projects/Medical%20records%20-%20NLP/#3-search-engine","title":"3. Search Engine","text":"<p>The code vectorizes the textual data using transformers, creating an embedding file. It calculates similarity scores using cosine similarity between the search query and the embeddings of the medical records. Finally, the code displays the similar terms and their context within the medical records.</p>"},{"location":"projects/Academic%20Projects/Medical%20records%20-%20NLP/#4-repository-structure","title":"4. Repository Structure","text":"<ul> <li><code>Dossier Analyse</code>: Contains the code using the CRF Tagger to extract named entities and structure them for JSON storage. Other scripts handle the Quaero database and CRF model training.</li> <li><code>Dossier Ordonnance</code>: Handles prescriptions, extracting named entities and structuring them into JSON files.</li> <li><code>Dossier FichePatient</code>: Processes patient records and develops regular expressions (Regex) for entity recognition.</li> <li><code>Dossier CR</code>: Processes radiology and scan reports, extracts named entities, and structures them into JSON files.</li> <li><code>Jsons</code>: Stores JSON files, including metadata and embeddings.</li> <li><code>Moteur de Recherche</code>: Contains all functionalities related to the search engine, including embeddings and similarity computations.</li> <li><code>Fichier Total Run</code>: Consolidates all functions for data extraction, NER, JSON creation, metadata embedding, and storage.</li> <li><code>P001</code>: Patient folder used for testing purposes.</li> </ul>"},{"location":"projects/Academic%20Projects/Medical%20records%20-%20NLP/#link-to-project-repository","title":"Link to Project Repository","text":""},{"location":"projects/Academic%20Projects/RAG%20with%20Langchain/","title":"Resume","text":"<p>The goal of this project is to build a RAG (Retrieval-Augmented Generation) pipeline that allows us to extract precise answers from selected Arxiv articles.  This solution can be adapted for other sources, whether PDF or non-PDF, while maintaining the same project steps.</p> <p>We tested multiple pipelines by combining various types of parsers (such as Langchain\u2019s ArxivLoader and LlmSherpa), different databases (Chroma, JSON, and LlamaIndex) to search within chunks, and various embedders (like Roberta, SentenceTransformer, and Text-Embedding Ada-002, among others).  Finally, we evaluated several LLMs (GPT 3.5, 4, 4 Turbo, and Gemini). The results and the details of the combinations are presented in the attached PDF presentation.</p>"},{"location":"projects/Academic%20Projects/RAG%20with%20Langchain/#the-rag-processus","title":"The RAG processus","text":"<p>Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with language generation models to provide more accurate and contextually relevant answers. Instead of relying solely on pre-trained language models, RAG enhances their performance by first retrieving relevant documents or data from an external knowledge base (such as PDFs, databases, or websites) and then using that retrieved information to generate responses.</p> <p>The process typically involves two steps:</p> <ol> <li> <p>Retrieval: The model retrieves relevant chunks of information from a predefined dataset or knowledge source based on a query or prompt.</p> </li> <li> <p>Generation: Using the retrieved information, a language model (like GPT or any other LLM) generates a coherent and precise response.</p> </li> </ol> <p>RAG improves the quality of answers, especially in scenarios where the language model alone might lack specific or up-to-date knowledge. This method is particularly useful for tasks such as answering questions from long documents, technical papers, or industry-specific data.</p> <p></p>"},{"location":"projects/Academic%20Projects/RAG%20with%20Langchain/#benchmark","title":"Benchmark","text":""},{"location":"projects/Academic%20Projects/RAG%20with%20Langchain/#pipelines","title":"Pipelines","text":"<p>In this project, you will find the script for pipelines 1 and 2, which represent my contributions to the hackathon.</p>"},{"location":"projects/Academic%20Projects/RAG%20with%20Langchain/#results","title":"Results","text":"<p>2. </p>"},{"location":"projects/Academic%20Projects/RAG%20with%20Langchain/#proof-of-concept","title":"Proof of concept","text":"<p>Link to project</p>"},{"location":"projects/Academic%20Projects/RealtimeDataStreaming/","title":"Real-Time Data Streaming Pipeline: Portfolio Project","text":""},{"location":"projects/Academic%20Projects/RealtimeDataStreaming/#project-overview","title":"Project Overview","text":"<p>I implemented a real-time data streaming pipeline to showcase my expertise in data engineering. This project demonstrates the complete workflow, from data ingestion to processing and storage, utilizing a modern stack of tools. The entire environment was containerized for scalability and ease of deployment.</p>"},{"location":"projects/Academic%20Projects/RealtimeDataStreaming/#key-achievements","title":"Key Achievements","text":"<ul> <li>Architected a robust pipeline using industry-leading tools such as Apache Airflow, Kafka, Spark, and Cassandra.  </li> <li>Orchestrated workflows with Apache Airflow, automating data ingestion from an API.  </li> <li>Set up a scalable streaming system with Apache Kafka and Zookeeper for real-time data processing.  </li> <li>Processed data efficiently with Apache Spark, handling real-time transformations.  </li> <li>Designed and implemented a dual storage solution:  </li> <li>Cassandra for scalable NoSQL storage.  </li> <li>PostgreSQL for relational data.  </li> <li>Containerized the full environment using Docker Compose, enabling seamless deployment across environments.  </li> </ul>"},{"location":"projects/Academic%20Projects/RealtimeDataStreaming/#tech-stack","title":"Tech Stack","text":"<ul> <li>Workflow Orchestration: Apache Airflow  </li> <li>Real-Time Streaming: Apache Kafka, Kafka Connect, Zookeeper  </li> <li>Data Processing: Apache Spark  </li> <li>Data Storage: Cassandra, PostgreSQL  </li> <li>Containerization: Docker, Docker Compose  </li> </ul>"},{"location":"projects/Academic%20Projects/RealtimeDataStreaming/#system-architecture","title":"System Architecture","text":"<ol> <li>Data Ingestion:    - Data is fetched from an external API using Airflow DAGs.  </li> <li>Data Streaming:    - Kafka brokers stream data into the pipeline.    - Zookeeper ensures distributed synchronization.  </li> <li>Data Processing:    - Spark processes the data in real time.  </li> <li>Data Storage:    - Cassandra stores high-throughput NoSQL data.    - PostgreSQL stores structured relational data for analytics.  </li> <li>Containerization:    - Docker Compose manages and orchestrates all services in an isolated environment.</li> </ol>"},{"location":"projects/Academic%20Projects/RealtimeDataStreaming/#results","title":"Results","text":"<p>This project demonstrates how to create a scalable and efficient real-time data streaming pipeline. The architecture can handle high-throughput data streams and supports both NoSQL and relational storage, making it adaptable for various business use cases.</p>"},{"location":"projects/Academic%20Projects/RealtimeDataStreaming/#learning-outcomes","title":"Learning Outcomes","text":"<ul> <li>Mastered the integration of multiple tools into a cohesive pipeline.  </li> <li>Gained hands-on experience in containerization with Docker.  </li> <li>Improved understanding of real-time data processing and storage solutions.  </li> </ul> <p>This project is a testament to my skills in data engineering, stream processing, and working with cutting-edge data technologies. It represents my ability to design and implement end-to-end solutions for real-world data challenges.</p>"},{"location":"projects/Academic%20Projects/RealtimeDataStreaming/#link-to-project","title":"Link to project","text":""},{"location":"projects/Professional%20Projects/","title":"Professional Projects","text":""},{"location":"projects/Professional%20Projects/#hello-friends","title":"Hello friends!","text":"<p>Hello colleagues and collaborators! I'm excited to showcase a selection of my professional projects, highlighting my expertise, problem-solving skills, and passion for data science. In this section, you'll gain insights into real-world challenges I've addressed and solutions I've implemented throughout my professional journey.</p>"},{"location":"projects/Professional%20Projects/#explore-my-projects","title":"Explore My Projects","text":"<p>Let's delve into a curated collection of professional projects that showcase my skills and achievements.</p>"},{"location":"projects/Professional%20Projects/#leave-a-comment","title":"Leave a Comment","text":"<p>Your insights and feedback are valuable. If you have any thoughts or questions about these professional projects, please leave a comment below. Let's engage in meaningful conversations!</p>"}]}