---
comments: true
---

# Real-Time Data Streaming Pipeline: Portfolio Project

### Project Overview

I implemented a **real-time data streaming pipeline** to showcase my expertise in data engineering. This project demonstrates the complete workflow, from data ingestion to processing and storage, utilizing a modern stack of tools. The entire environment was containerized for scalability and ease of deployment.

### Key Achievements

- **Architected a robust pipeline** using industry-leading tools such as Apache Airflow, Kafka, Spark, and Cassandra.  
- **Orchestrated workflows** with **Apache Airflow**, automating data ingestion from an API.  
- **Set up a scalable streaming system** with **Apache Kafka** and **Zookeeper** for real-time data processing.  
- **Processed data** efficiently with **Apache Spark**, handling real-time transformations.  
- **Designed and implemented a dual storage solution**:  
  - **Cassandra** for scalable NoSQL storage.  
  - **PostgreSQL** for relational data.  
- **Containerized the full environment** using **Docker Compose**, enabling seamless deployment across environments.  

### Tech Stack

- **Workflow Orchestration**: Apache Airflow  
- **Real-Time Streaming**: Apache Kafka, Kafka Connect, Zookeeper  
- **Data Processing**: Apache Spark  
- **Data Storage**: Cassandra, PostgreSQL  
- **Containerization**: Docker, Docker Compose  


### System Architecture

1. **Data Ingestion**:  
   - Data is fetched from an external API using Airflow DAGs.  
2. **Data Streaming**:  
   - Kafka brokers stream data into the pipeline.  
   - Zookeeper ensures distributed synchronization.  
3. **Data Processing**:  
   - Spark processes the data in real time.  
4. **Data Storage**:  
   - Cassandra stores high-throughput NoSQL data.  
   - PostgreSQL stores structured relational data for analytics.  
5. **Containerization**:  
   - Docker Compose manages and orchestrates all services in an isolated environment.

### Results

This project demonstrates how to create a scalable and efficient real-time data streaming pipeline. The architecture can handle high-throughput data streams and supports both NoSQL and relational storage, making it adaptable for various business use cases.

### Learning Outcomes

- Mastered the integration of multiple tools into a cohesive pipeline.  
- Gained hands-on experience in containerization with Docker.  
- Improved understanding of real-time data processing and storage solutions.  

This project is a testament to my skills in data engineering, stream processing, and working with cutting-edge data technologies. It represents my ability to design and implement end-to-end solutions for real-world data challenges.

### [Link to project](https://github.com/ZinebAissaoui/RealtimeDataStreaming)